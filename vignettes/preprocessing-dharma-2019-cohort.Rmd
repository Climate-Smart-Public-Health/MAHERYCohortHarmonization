---
title: "Preprocessing Dharma 2019 Cohort"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Preprocessing Dharma 2019 Cohort}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r}
library(MAHERYCohortHarmonization)
```

<!-- WARNING - This vignette is generated by {fusen} from dev/flat_preprocess_2019.Rmd: do not edit by hand -->




# preprocess_srp2019

Similar to the previous notebook, we are going to preprocess the 2019 dharma data in this notebook.
The goal is to create a function that will take the raw data and return a preprocessed version of the cohort
to merge with the 2018 cohort. As before, we use the column suffix `_clean`
to denote that the column has been cleaned and is being kept for analysis.
```{r}
library(here)
library(dplyr)
library(tidyr)
library(stringr)
#library(janitor)
library(targets)
library(skimr)
library(forcats)
library(lubridate)
library(googledrive)
library(ggplot2)
library(ggalluvial)
library(DT)
```


## Data Structures

The data for the dharma 2019 collection is stored in an excel file with multiple sheets. As before,
we use the googledriver package to monitor and download it for access. In our targets pipeline,
this file was called `Dharma_followup_2020plus.xlsx` and was tracked 

First, we are going to load the data and do some basic EDA to see how the dharma data collection differs from
the opensrp data collection.
```{r}
tar_load(dharma2019, store = here::here("_targets"))
```


Like the 2018 data, the dharma 2019 data is stored in a list of data frames corresponding to sheets in the excel.

```
Level 0: Household enrollment 
Level 1: Individuals in household
Level 2: Antropometric data (every 3 months), dietary intake data collected from head of household (every 4 months), followup data.
```

We have a data dictionary that we can reference:
```{r}
dharma2019_dict <- dharma2019$`Data Dictionary`
dharma2019_dict %>%
  fill(level, .direction = "down") -> dharma2019_dict

level_1_dict <- dharma2019_dict %>%
  filter(level == 1)
level_1_dict
```


This should hopefully be a tidy reference for the cohort level data.

Our goal is to see the final structure of the cohort match this:
```{r}
tar_read("cohort_2018_deid", store = here::here("_targets")) %>%
  head()
```


We'll use the `Level 1` sheet for the individual data.
```{r}
cohort_df <- dharma2019$`Level 1 Named`
```


There's a column called `deleted` that we should
probably inspect:
```{r}
cohort_df %>%
  select(deleted) %>%
  table()
```


105 records removed isn't too many, so I'm willing to trust that
these are missing not at random. We can check
these against the missingness in the gender column:
```{r}
cohort_df %>%
  select(deleted, L1_q8_gender) %>%
  table()
```


The majority of the time we have a deleted record,
we also have a "." in gender. So it's safe to assume
we can go off of that column.

> For the purposes of getting household heads, we need to
make a big assumption that the column `fact_0_id` is the
corresponding head of household ID. However, this column
is sparse, ie has one value and then a lot of NAs below until
the next value. This is a big assumption but if it is true,
we can use `fill()` to fill down the values and get the household head
ID for each individual.

```{r}
# some na filtering
cohort_df %>%
  fill(fact_0_id, .direction = "down") %>%
  filter(!is.na(L1_q39_date)) %>%
  filter(L1_q39_date != ".") %>% 
  filter(!is.na(L1_q1_name)) %>%
  filter(L1_q1_name != ".") %>% 
  filter(!is.na(L1_q1_name)) %>%
  filter(L1_q1_name != ".") %>% 
  filter(!is.na(L1_q8_gender)) %>%
  filter(L1_q8_gender != ".") %>%
  filter(!is.na(L1_q2_dob_year)) %>%
  filter(L1_q2_dob_year != ".") -> cohort_df_
```


## Household Head Name

First, we need to get a dataset with the household head name, so that
we can merge it into the cohort data.
```{r}
household_df <- dharma2019$`Level 0 Named` %>%
  select(fact_0_id, L0_q2_hh_name) %>%
  rename(hh_head_name = L0_q2_hh_name) %>%
  distinct() %>%
  mutate(hh_head_name = na_if(hh_head_name,".")) %>%
  filter(!is.na(hh_head_name))

cohort_df_ <- cohort_df_ %>%
  left_join(household_df, by = "fact_0_id")
```


## Name Cleaning

To find the enrollee's name, we use Level 1's name data:
```{r}
cohort_df_ %>%
  select(L1_q1_name) %>%
  table()
```


Let's first clean these names to remove extraneous whitespace and have it all
in lowercase:
```{r}
cohort_df_ <- cohort_df_ %>%
  mutate(
    name_clean = L1_q1_name %>%
      str_to_lower() %>%
      str_replace("\\.", " ") %>% # remove the period from names; I don't know why it was allowed in
      str_squish()
    )
```


We need to figure out who the duplicates are in the dataset.
```{r}
cohort_df_ %>%
  #select(L1_q1_name) %>%
  group_by(name_clean) %>%
  summarise(n = n()) %>%
  filter(n > 1) %>% pull(name_clean) -> duplicate_names

cohort_df_ %>%
  filter(name_clean %in% duplicate_names) %>%
  arrange(name_clean) %>%
  select(contains("clean"), everything())
```


It's encouraging that the duplicates have different IDs and household head names. Let's move forward with this.
    
  
  

An unresolved question: what does the capitalization signify?

## DoB

Next we'll handle the date of birth,
similar to how it was done in the 2018 cohort. We can run it
as-is and see the errors:
```{r}
cohort_df_ %>%
  mutate(
    dob_date = L1_q5_dob_date,
    dob_year = L1_q2_dob_year,
    dob_month = L1_q4_dob_month,
    dob_actual = L1_q6_actual_dob
  ) %>%
  clean_opencensus_dob() -> cohort_df_
  
cohort_df_ %>%
  select(contains("clean"), contains("dob")) %>%
  select(-name_clean) %>%
  filter(is.na(dob_clean)) -> dob_errors

cohort_df_ %>%
  select(dob_clean) %>%
  summary()
```



Notably, some of these are expected errors, such as 31 September,
29 February in 1997, and so on. So we will assume these are
acceptable errors and proceed with the rest of the data cleaning.
```{r}
datatable(dob_errors)
```


## Ethnicity

Next, we deal with ethnicity, which fortunately is pretty clean already:
```{r}
cohort_df_ %>% 
  mutate(ethnicity_clean = str_to_lower(L1_q10_ethnicity)) -> cohort_df_
```


```{r}
cohort_df_ %>%
  select(ethnicity_clean) %>%
  table()
```


## Education

This is similarly simple, and we can use the same encoding and function as the 2018 cohort:
```{r}
cohort_df_ %>%
  mutate(education = str_to_sentence(L1_q169_education) %>% str_replace("Universite", "University")) %>%
  recode_opensrp_education() %>%
  select(education, education_level_clean) %>% table()
```


We'll replace the missing values with NAs, as they must be missing not at random:
```{r}
cohort_df_ %>%
  mutate(education = str_to_sentence(L1_q169_education) %>% str_replace("Universite", "University") %>% str_replace("\\.|No education", "None")) %>%
  recode_opensrp_education() -> cohort_df_
```


## Profession

We don't have a prepared function for this, but it is fairly easy to do:
```{r}
prof_columns <- names(cohort_df_) %>%
  str_subset(., "what_occupation|other_occupation")

cohort_df_ %>%
  select(all_of(prof_columns)) %>%
  mutate(across(everything(), ~ str_replace(., "\\.|Other", NA_character_))) %>%
  unite(occupation, everything(), na.rm=TRUE) %>%
  distinct()
```


These are all the options for professions. Unlike the OpenSRP data, 
we can see that respondents were allowed to select multiple professions.
We'll leave these as underscore separated values.

Our function for processing professions will also translate
the Malagasy professions to English, so that we can have a consistent
set of professions across the cohorts.
    
  
  

```{r}
cohort_df_ <- clean_2019_profession(cohort_df_)
```

  
```{r}
mapping <- audit_categorical_cleaning(
  cohort_df_,
  occupation,
  profession_clean
)

datatable(mapping)
```


## Sex and Marital Status

Next we deal with the sex and marital status data:
```{r}
cohort_df_ %>%
  mutate(
    sex_clean = str_to_lower(L1_q8_gender) %>%
     str_replace("\\.", NA_character_) %>% 
     as.factor()
  ) -> cohort_df_
```


```{r}
cohort_df_ %>%
  select(sex_clean) %>%
  table()
```


```{r}
cohort_df_ %>%
  mutate(
    marital_status_clean = str_to_lower(L1_q15_martial_stat) %>%
      str_replace("\\.", NA_character_) %>%
      as.factor()
  ) -> cohort_df_
```


```{r}
cohort_df_ %>%
  select(marital_status_clean) %>%
  table()
```


## Household Heads

Lastly, we clean the household heads name to make sure we can salt an individual's UID:
```{r}
cohort_df_ %>%
  mutate(
    hh_head_name_clean = hh_head_name %>%
      str_to_lower() %>%
      str_replace("\\.", " ") %>% # remove the period from names; I don't know why it was allowed in
      str_squish()
  ) -> cohort_df_
```


We won't have to clean this separately, so
this can stay in the same function.

And with that, the basic demographic cohort is complete.

## Pipeline Summary

To run this step of the project, we wrap up the work in the function
that will be called in the targets pipeline with the following steps:

- We filter NA rows for anyone who has no name, ethnicity, or date of birth.
- We clean the names to remove periods and extra whitespace.
- We clean the date of birth to ensure it is in a consistent format
- We clean ethnicity, education, and profession, creating a new function
for profression cleaning since it requires translations
- Sex and marital status are also included but require no major changes



